{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2: Implementing Backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from utils import UClasses\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preliminaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = UClasses(n=1000)\n",
    "ds.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Operation` class: Activation/Loss functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Operation(object):\n",
    "    '''\n",
    "     Operation class\n",
    "     \n",
    "     This is the abstract base class that other operations should be based on.\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def __call__(self, x):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def derivative(self):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Identity(Operation):\n",
    "    '''\n",
    "     act = Identity()\n",
    "     \n",
    "     Creates an Operation object that represents the identity mapping.\n",
    "     \n",
    "     Usage:\n",
    "      act = Identity()\n",
    "      act(np.array([[1.2, 5.]]))\n",
    "     produces the numpy array\n",
    "      [[1.2, 5.]]\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        return\n",
    "        \n",
    "    def __call__(self, z):\n",
    "        '''\n",
    "         y = act(z)\n",
    "         \n",
    "         Evaluates the identity function, element-by-element, on z.\n",
    "         \n",
    "         Input:\n",
    "          z  is a numpy array\n",
    "         Output:\n",
    "          y  is a numpy array the same size as z\n",
    "        '''\n",
    "        self.dims = z.shape\n",
    "        y = copy.deepcopy(z)\n",
    "        return y\n",
    "    \n",
    "    def derivative(self, s=None):\n",
    "        '''\n",
    "         act.derivative(s=None)\n",
    "         \n",
    "         Computes the derivative of the identity mapping\n",
    "         element-by-element.\n",
    "         Note that the __call__ function must be called before this\n",
    "         function can be called.\n",
    "         \n",
    "         Input:\n",
    "           s       array the same size as z, which multiplies the\n",
    "                   derivative\n",
    "           \n",
    "         Output:\n",
    "           dactdz  array the same size as z when __call__ was called,\n",
    "                   and is s times the derivative\n",
    "           \n",
    "         Usage:\n",
    "           dactdz = act.derivative()\n",
    "           dactdz = act.derivative(s)\n",
    "        '''\n",
    "        # Compute the derivatives\n",
    "        if s is None:\n",
    "            return np.ones(self.dims)\n",
    "        else:\n",
    "            return s\n",
    "        \n",
    "class Logistic(Operation):\n",
    "    '''\n",
    "     act = Logistic()\n",
    "     \n",
    "     Creates an Operation object that represents the logistic\n",
    "     function.\n",
    "     \n",
    "     Usage:\n",
    "      act = Logistic()\n",
    "      act(np.array([0., 0.5]))\n",
    "     produces the numpy array\n",
    "      [0.5 , 0.62245933]\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        return\n",
    "        \n",
    "    def __call__(self, z):\n",
    "        '''\n",
    "         y = act(z)\n",
    "         \n",
    "         Evaluates the logistic function, element-by-element, on z.\n",
    "         \n",
    "         Input:\n",
    "          z  is a numpy array\n",
    "         Output:\n",
    "          y  is a numpy array the same size as z\n",
    "        '''\n",
    "        self.dims = z.shape\n",
    "        self.n_samples = np.shape(z)[0]\n",
    "        self.dims = np.shape(z)[-1]\n",
    "        # Logistic forumla [!]\n",
    "        self.y = 1. / (1. + np.exp(-z))  # Used for derivative [!]\n",
    "        return self.y\n",
    "    \n",
    "    def derivative(self, s=None):\n",
    "        '''\n",
    "         act.derivative(s=None)\n",
    "         \n",
    "         Computes the derivative of the logistic function\n",
    "         element-by-element.\n",
    "         Note that the __call__ function must be called before this\n",
    "         function can be called.\n",
    "         \n",
    "         Input:\n",
    "           s       array the same size as z, which multiplies the\n",
    "                   derivative\n",
    "           \n",
    "         Output:\n",
    "           dactdz  array the same size as z when __call__ was called,\n",
    "                   and is s times the derivative\n",
    "\n",
    "         Usage:\n",
    "           dactdz = act.derivative()\n",
    "           dactdz = act.derivative(s)\n",
    "        '''\n",
    "        # Derivatives of logistic [!]\n",
    "        if s is None:\n",
    "            return self.y * (1. - self.y)\n",
    "        else:\n",
    "            return s * self.y * (1. - self.y)\n",
    "\n",
    "class Softmax(Operation):\n",
    "    '''\n",
    "     act = Softmax()\n",
    "     Creates an Operation object that represents the softmax\n",
    "     function. The softmax is applied to the rows of the input.\n",
    "     Usage:\n",
    "      act = Softmax()\n",
    "      act(np.array([[0., 0.5]]))\n",
    "     produces the numpy array\n",
    "      [0.5 , 0.62245933]\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        return\n",
    "    def __call__(self, z):\n",
    "        v = np.exp(z)\n",
    "        denom = np.sum(v, axis=1)\n",
    "        self.y = v/np.tile(denom[:,np.newaxis], [1,np.shape(v)[1]])\n",
    "        return self.y\n",
    "    def derivative(self, s):\n",
    "        '''\n",
    "         act.derivative(s)\n",
    "         Computes and the derivative of the softmax function.\n",
    "         Note that the __call__ function must be called before this\n",
    "         function can be called.\n",
    "         Input:\n",
    "           s       array the same size as z, which multiplies the\n",
    "                   derivative\n",
    "                   NOTE: s is a mandatory argument (not optional)\n",
    "                   NOTE: s should have only a single non-zero element\n",
    "         Output:\n",
    "           dactdz  array the same size as z when __call__ was called,\n",
    "                   and is s times the derivative\n",
    "         Usage:\n",
    "           dactdz = act.derivative(s)\n",
    "        '''\n",
    "        idx = np.nonzero(s)[1]\n",
    "        s_gamma = np.zeros_like(s)\n",
    "        y_gamma = np.zeros_like(self.y)\n",
    "        kronecker = np.zeros_like(s)\n",
    "        for k,gamma in enumerate(idx):\n",
    "            s_gamma[k,:] = s[k,gamma]\n",
    "            y_gamma[k,:] = self.y[k,gamma]\n",
    "            kronecker[k,gamma] = 1.\n",
    "        dydz = s_gamma*y_gamma*(kronecker-self.y)\n",
    "        return dydz\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSE(Operation):\n",
    "    '''\n",
    "     E = MSE()\n",
    "     \n",
    "     Creates an object that implements the mean squared error loss.\n",
    "     \n",
    "     Usage:\n",
    "      E = MSE()\n",
    "      loss = E(y, t)\n",
    "      \n",
    "     Example:\n",
    "      y = np.array([[0.5, 0.1],[-0.4, 0.9], [-0.1, 0.4]])\n",
    "      t = np.array([[0.6, 0.1],[-0.4, 0.7], [-0.1, 0.6]])\n",
    "      loss = E(y, t)\n",
    "     produces the value\n",
    "      0.015  since it equals\n",
    "      (0.1^2 + 0.2^2 + 0.2^2)/2 / 3\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        self.dE = []\n",
    "    \n",
    "    def __call__(self, y, t):\n",
    "        '''\n",
    "         E.__call__(y, t)  or   E(y, t)\n",
    "         \n",
    "         Computes the mean (average) squared error between the outputs\n",
    "         y and the targets t.\n",
    "         \n",
    "         Inputs:\n",
    "           y  array with one sample per row\n",
    "           t  array the same size as y\n",
    "           \n",
    "         Output:\n",
    "           loss  MSE loss (scalar)\n",
    "        '''\n",
    "        # MSE formula\n",
    "        self.n_samples = np.shape(t)[0]\n",
    "        E = np.sum((y-t)**2)/2./self.n_samples\n",
    "        self.dE = (y-t) / self.n_samples\n",
    "        return E\n",
    "\n",
    "    def derivative(self):\n",
    "        '''\n",
    "         E.derivative()\n",
    "         \n",
    "         Computes and the derivative of the MSE with respect to y.\n",
    "         Note that the __call__ function must be called before this\n",
    "         function can be called.\n",
    "         \n",
    "         Output:\n",
    "           dEdy  array the same size as y when __call__ was called\n",
    "        '''\n",
    "        # Compute the gradient of MSE w.r.t. output\n",
    "        return self.dE\n",
    "\n",
    "        \n",
    "class CrossEntropy(Operation):\n",
    "    '''\n",
    "     E = CrossEntropy()\n",
    "     \n",
    "     Creates an object that implements the average cross-entropy loss.\n",
    "     \n",
    "     Usage:\n",
    "      E = CrossEntropy()\n",
    "      loss = E(y, t)\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        self.dE = []\n",
    "    \n",
    "    def __call__(self, y, t):\n",
    "        '''\n",
    "         E.__call__(y, t)  or   E(y, t)\n",
    "         \n",
    "         Computes the average cross-entropy between the outputs\n",
    "         y and the targets t.\n",
    "         \n",
    "         Inputs:\n",
    "           y  array with one sample per row\n",
    "           t  array the same size as y\n",
    "           \n",
    "         Output:\n",
    "           loss  average CE loss (scalar)\n",
    "        '''\n",
    "        n_samples, dim = np.shape(t)\n",
    "        # Cross Entropy formula [!]\n",
    "        # Must divide by the number of samples [!]\n",
    "        E = -np.sum(t*np.log(y)+(1.-t)*np.log(1.-y))/n_samples\n",
    "        self.dE = (y-t) / y / (1.-y) /n_samples  # Used for derivative\n",
    "        return E\n",
    "\n",
    "    def derivative(self):\n",
    "        '''\n",
    "         E.derivative()\n",
    "         \n",
    "         Computes and the derivative of cross-entropy with respect to y.\n",
    "         Note that the __call__ function must be called before this\n",
    "         function can be called.\n",
    "         \n",
    "         Output:\n",
    "           dEdy  array the same size as y when __call__ was called\n",
    "        '''\n",
    "        # Compute the gradient of CE w.r.t. output\n",
    "        return self.dE\n",
    "        \n",
    "\n",
    "class CategoricalCE(Operation):\n",
    "    def __init__(self):\n",
    "        return\n",
    "    def __call__(self, y, t):\n",
    "        self.t = t\n",
    "        self.y = y\n",
    "        return -np.sum(t * np.log(y)) / len(t)\n",
    "    def derivative(self):\n",
    "        return -self.t/self.y / len(self.t)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Layer` Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer(object):\n",
    "    def __init__(self):\n",
    "        return\n",
    "\n",
    "    def __call__(self, x):\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "class Population(Layer):\n",
    "    '''\n",
    "     lyr = Population(nodes, act=Identity())\n",
    "\n",
    "     Creates a Population layer object.\n",
    "\n",
    "     Inputs:\n",
    "       nodes  the number of nodes in the population\n",
    "       act    activation function (Operation object)\n",
    "       \n",
    "     Usage:\n",
    "       lyr = Population(3, act=Logistic())\n",
    "       h = lyr(z)\n",
    "       print(lyr())   # prints current value of lyr.h\n",
    "    '''\n",
    "\n",
    "    def __init__(self, nodes, act=Identity()):\n",
    "        self.nodes = nodes\n",
    "        self.z = None\n",
    "        self.h = None\n",
    "        self.act = act\n",
    "        self.params = []\n",
    "\n",
    "    def __call__(self, x=None):\n",
    "        if x is not None:\n",
    "            self.z = x\n",
    "            self.h = self.act(x)\n",
    "        return self.h\n",
    "\n",
    "\n",
    "class Connection(Layer):\n",
    "    '''\n",
    "     lyr = Connection(from_nodes=1, to_nodes=1)\n",
    "\n",
    "     Creates a layer of all-to-all connections.\n",
    "\n",
    "     Inputs:\n",
    "       from_nodes  number of nodes in source layer\n",
    "       to_nodes    number of nodes in receiving layer\n",
    "\n",
    "     Usage:\n",
    "       lyr = Connection(from_nodes=3, to_nodes=5)\n",
    "       z = lyr(h)\n",
    "       lyr.W    # matrix of connection weights\n",
    "       lyr.b    # vector of biases\n",
    "    '''\n",
    "\n",
    "    def __init__(self, from_nodes=1, to_nodes=1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.W = np.random.randn(from_nodes, to_nodes) / np.sqrt(from_nodes)\n",
    "        self.b = np.zeros(to_nodes)\n",
    "        self.params = [self.W, self.b]\n",
    "\n",
    "    def __call__(self, x=None):\n",
    "        if x is None:\n",
    "            print('Should not call Connection without arguments.')\n",
    "            return\n",
    "        P = len(x)\n",
    "        if P>1:\n",
    "            return x@self.W + np.outer(np.ones(P), self.b)\n",
    "        else:\n",
    "            return x@self.W + self.b\n",
    "\n",
    "\n",
    "class DenseLayer(Layer):\n",
    "    '''\n",
    "     lyr = DenseLayer(from_nodes=1, to_nodes=1, act=Logistic())\n",
    "\n",
    "     Creates a DenseLayer object, composed of 2 layer objects:\n",
    "       L1  a Connection layer of connection weights, and\n",
    "       L2  a Population layer, consisting of nodes that receives current\n",
    "           from the Connection layer, and apply the activation function\n",
    "\n",
    "     Inputs:\n",
    "       from_nodes  how many nodes are in the layer below\n",
    "       to_nodes    how many nodes are in the new Population layer\n",
    "       act         activation function (Operation object)\n",
    "       \n",
    "     Usage:\n",
    "       lyr = DenseLayer(from_nodes=3, to_nodes=5)\n",
    "       h2 = lyr(h1)\n",
    "       lyr.L1.W        # connection weights\n",
    "       lyr.L2()        # activities of layer\n",
    "       lyr.L2.act      # activation function of layer\n",
    "    '''\n",
    "\n",
    "    def __init__(self, from_nodes=1, to_nodes=1, act=Logistic()):\n",
    "        self.L1 = Connection(from_nodes=from_nodes, to_nodes=to_nodes)\n",
    "        self.L2 = Population(from_nodes, act=act)\n",
    "\n",
    "    def __call__(self, x=None):\n",
    "        if x is None:\n",
    "            return self.L2.h\n",
    "        else:\n",
    "            # Calculate and return the operation of the two layers, L1 and L2\n",
    "            return self.L2(self.L1(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `Network` Class (*action required*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(object):\n",
    "    '''\n",
    "     net = Network()\n",
    "\n",
    "     Creates a Network object.\n",
    "     \n",
    "     Usage:\n",
    "       net = Network()\n",
    "       net.add_layer(L)\n",
    "       ... (add more layers)\n",
    "       y = net(x)\n",
    "       net.lyr[1]    # reference to Layer object\n",
    "    '''\n",
    "\n",
    "    def __init__(self):\n",
    "        self.lyr = []\n",
    "        self.loss = None\n",
    "\n",
    "    def add_layer(self, L):\n",
    "        '''\n",
    "         net.add_layer(L)\n",
    "         \n",
    "         Adds the layer object L to the network.\n",
    "         \n",
    "         Note: It is up to the user to make sure the Layer object\n",
    "               fits with adjacent layers.\n",
    "        '''\n",
    "        self.lyr.append(L)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        '''\n",
    "         y = net(x)\n",
    "         \n",
    "         \n",
    "         Input:\n",
    "           x  batch of inputs, one input per row\n",
    "           \n",
    "         Output:\n",
    "           y  corresponding outputs, one per row\n",
    "        '''\n",
    "        for l in self.lyr:\n",
    "            x = l(x)\n",
    "        return x\n",
    "\n",
    "    def learn(self, ds, lrate=1., epochs=10):\n",
    "        '''\n",
    "         net.Learn(ds, lrate=1., epochs=10)\n",
    "\n",
    "         Runs backprop on the network, training on the data from\n",
    "         the Dataset object ds.\n",
    "         \n",
    "         Inputs:\n",
    "           ds       a Dataset object\n",
    "           lrate    learning rate\n",
    "           epochs   number of epochs to run\n",
    "        '''\n",
    "        #\n",
    "        # Did they call backprop each epoch?\n",
    "        #\n",
    "        x = ds.inputs()\n",
    "        t = ds.targets()\n",
    "        loss_history = []\n",
    "        for epoch in range(epochs):\n",
    "            y = self(x)\n",
    "            self.backprop(t, lrate=lrate)\n",
    "            # This part is not necessary\n",
    "            if epoch%100==0:\n",
    "                # Report progress\n",
    "                cost = self.loss(y, t)\n",
    "                loss_history.append(cost)\n",
    "                print(f'{epoch}: cost = {cost}')\n",
    "                \n",
    "        return np.array(loss_history)\n",
    "\n",
    "\n",
    "    def backprop(self, t, lrate=1.):\n",
    "        '''\n",
    "         net.backprop(t, lrate=1.)\n",
    "         \n",
    "         Using the error between the state of the output layer and\n",
    "         the connection weights and biases.\n",
    "         \n",
    "         NOTE: This method assumes that the network is in the\n",
    "         \n",
    "         Inputs:\n",
    "           t      batch of targets, one per row\n",
    "           lrate  learning rate\n",
    "        '''\n",
    "        #\n",
    "        # Are the backprop equations correct?\n",
    "        # Does it call derivative() functions?\n",
    "        # Are the weights and biases updated correctly?\n",
    "        #\n",
    "        # Set up for computing the top gradient.\n",
    "        y = self.lyr[-1]()\n",
    "        loss = self.loss(y,t)\n",
    "        dEdh = self.loss.derivative()\n",
    "        # Work our way down through the layers\n",
    "        for i in range(len(self.lyr)-1, 0, -1):\n",
    "            # References to the layer below, and layer above\n",
    "            pre = self.lyr[i-1]   # layer below, (i-1)\n",
    "            post = self.lyr[i]    # layer above, (i)\n",
    "            # Note that:\n",
    "            #   post.L1.W contains the connection weights\n",
    "            #   post.L1.b contains the biases\n",
    "            #   post.L2.z contains the input currents\n",
    "            #   post.L2.h contains the upper layer's activities\n",
    "            # Compute dEdz from dEdh\n",
    "            #dEdz = dEdh\n",
    "            dEdz = post.L2.act.derivative(dEdh)\n",
    "            # Parameter gradients\n",
    "            #dEdW = np.zeros_like(post.L1.W)\n",
    "            #dEdb = np.zeros_like(post.L1.b)\n",
    "            dEdW = pre().T @ dEdz\n",
    "            dEdb = np.sum(dEdz, axis=0)\n",
    "            # Project gradient through connection, to layer below\n",
    "            #dEdh = np.zeros_like(pre.h)\n",
    "            dEdh = dEdz @ post.L1.W.T\n",
    "            # Update weight parameters\n",
    "            post.L1.W = post.L1.W - lrate*dEdW\n",
    "            post.L1.b = post.L1.b - lrate*dEdb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Your code should work below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Network()\n",
    "\n",
    "# Create layers\n",
    "input_layer = Population(2)\n",
    "h1 = DenseLayer(from_nodes=2, to_nodes=30, act=Logistic())\n",
    "h2 = DenseLayer(from_nodes=30, to_nodes=10, act=Logistic())\n",
    "output_layer = DenseLayer(from_nodes=10, to_nodes=2, act=Logistic())\n",
    "\n",
    "# Add layers to the network, from bottom to top\n",
    "net.add_layer(input_layer)\n",
    "net.add_layer(h1)\n",
    "net.add_layer(h2)\n",
    "net.add_layer(output_layer)\n",
    "\n",
    "# Choose a loss function\n",
    "net.loss = CrossEntropy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Train the network\n",
    "loss_history = net.learn(ds, epochs=5000);\n",
    "\n",
    "# Plot the progress of the cost\n",
    "plt.plot(loss_history);\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Cost');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check, to see if output match targets\n",
    "y = net(ds.inputs())\n",
    "print(f'Outputs:\\n{y[:5,:]}')\n",
    "print(f'Targets:\\n{ds.targets()[:5,:]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Cluster plot to make us feel accomplished!\n",
    "ds.plot(labels=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate your model (*action required*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y, t):\n",
    "    '''\n",
    "     ac = accuracy(y, t)\n",
    "     \n",
    "     Calculates the fraction of correctly classified samples.\n",
    "     A sample is classified correctly if the largest element\n",
    "     in y corresponds to where the 1 is in the target.\n",
    "     \n",
    "     Inputs:\n",
    "       y  a batch of outputs, with one sample per row\n",
    "       t  the corresponding batch of targets\n",
    "       \n",
    "     Output:\n",
    "       ac the fraction of correct classifications (0<=ac<=1)\n",
    "    '''\n",
    "    #\n",
    "    # Are the errors/successes counted correctly?\n",
    "    # Is the answer divided by the number of samples?\n",
    "    #\n",
    "    true_class = np.argmax(t, axis=1)       # vector of indices for true class\n",
    "    estimated_class = np.argmax(y, axis=1)  # vector of indices for estimated class\n",
    "    errors = sum(true_class==estimated_class)  # add up how many times they match\n",
    "    acc = errors / len(ds)    # divide by the total number of samples\n",
    "    return acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ac = accuracy(net(ds.inputs()), ds.targets())\n",
    "print(f'Model training accuracy = {ac*100}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
